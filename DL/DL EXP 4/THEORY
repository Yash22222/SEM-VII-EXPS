AIM - Apply any of the following learning algorithms to learn the parameters of the supervised single-layer feed-forward neural network,
1. Stochastic Gradient Descent
2. Mini Batch Gradient Descent
3. Momentum GD

THEORY - 
A single-layer feedforward neural network (SLFN) is one of the simplest forms of artificial neural networks. It consists of an input layer, a single hidden layer, and an output layer. 
The aim of training such a network is to learn the parameters (weights and biases) that best map the input data to the corresponding outputs.

Gradient Descent Algorithms

1. Batch Gradient Descent (BGD):-
In Batch Gradient Descent, the algorithm computes the gradient of the cost function to the weights and biases for the entire dataset before updating the parameters. 
This approach ensures a stable convergence but can be computationally expensive and slow, especially for large datasets.

2. Stochastic Gradient Descent (SGD):-
In Stochastic Gradient Descent, the algorithm updates the parameters for each training example individually. This method introduces more noise in the learning process, which can help escape local minima but may also lead to less stable convergence. 
It is computationally more efficient compared to Batch Gradient Descent, especially for large datasets.

3. Mini-Batch Gradient Descent (MBGD):-
Mini-Batch Gradient Descent is a compromise between Batch and Stochastic Gradient Descent. Instead of using the entire dataset or a single training example, it uses a small batch of training examples to compute the gradient and update the parameters. 
This method provides a good balance between Batch Gradient Descent's convergence speed and Stochastic Gradient Descent's noise.

4. Momentum Gradient Descent (MGD):-
Momentum Gradient Descent is an extension of the basic gradient descent algorithms. It introduces a momentum term that helps accelerate gradients in the right direction and dampen oscillations. 
The momentum term adds a fraction of the previous update to the current update, making it more likely to escape local minima and converge faster.

CONCLUSION -
In the given experiment, the parameters of a supervised single-layer feedforward neural network were learned using three different gradient descent algorithms: 
Batch Gradient Descent (BGD), Stochastic Gradient Descent (SGD), and Mini-Batch Gradient Descent (MBGD).
Batch Gradient Descent showed stable and smooth convergence towards the minimum, though it was slower compared to the other methods.
Stochastic Gradient Descent converged faster due to more frequent updates but exhibited more fluctuations and noise in the loss function.
Mini-Batch Gradient Descent provided a middle ground with faster convergence than BGD and less noise than SGD.
All three methods eventually converged to similar values for the weights, biases, and final loss, demonstrating their effectiveness in training a simple linear model. However, depending on the size of the dataset and the need for computational efficiency, one might choose one algorithm over the others. For large datasets, Mini-Batch Gradient Descent is often the preferred choice due to its balance between speed and stability.

These methods form the foundation for training more complex neural networks, and understanding their nuances is crucial for developing efficient and effective machine learning models.
