{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP EXPERIMENT 2**\n",
        "## **Yash Ashok Shirsath BE AI&DS 65**\n",
        "\n",
        "\n",
        "### **AIM - Apply Various Text Pre Processing Techniques for any Given Text:- Tokenization and Filtration &amp; Script Validation.**"
      ],
      "metadata": {
        "id": "D7Wo8e70Zxla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Text :- 22222 Hazrat Nizamuddin - Mumbai CSMT Rajdhani Express Via Agra, Jhansi, Bhopal, Jalgaon, Nashik, Kalyan"
      ],
      "metadata": {
        "id": "anQdyTeWa3z5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1:- Tokenization**\n",
        "Tokenization is the process of splitting the text into individual words or tokens."
      ],
      "metadata": {
        "id": "BP4Y0-lsbWnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKwRCnWgZwsU",
        "outputId": "94bcb76e-ff2b-40d7-a137-dddf22f3d975"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"22222 Hazrat Nizamuddin - Mumbai CSMT Rajdhani Express Via Agra, Jhansi, Bhopal, Jalgaon, Nashik, Kalyan\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKqW62-ubiro",
        "outputId": "3daac418-8eca-4060-8a09-5fe827c95359"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['22222', 'Hazrat', 'Nizamuddin', '-', 'Mumbai', 'CSMT', 'Rajdhani', 'Express', 'Via', 'Agra', ',', 'Jhansi', ',', 'Bhopal', ',', 'Jalgaon', ',', 'Nashik', ',', 'Kalyan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2:- Filtration**"
      ],
      "metadata": {
        "id": "YwY-PdHgcfeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string"
      ],
      "metadata": {
        "id": "jOxYpgWJbqQG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPSv41xicqmO",
        "outputId": "b036fe2d-c991-45cd-f584-7e9994a4410e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens = [word for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nseq-9b9cyYM",
        "outputId": "b1e18984-15fe-492e-8935-5452613009fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['22222', 'Hazrat', 'Nizamuddin', 'Mumbai', 'CSMT', 'Rajdhani', 'Express', 'Via', 'Agra', 'Jhansi', 'Bhopal', 'Jalgaon', 'Nashik', 'Kalyan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3:- Script Validation**"
      ],
      "metadata": {
        "id": "GNCSHcCUc_IQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata"
      ],
      "metadata": {
        "id": "n41p6cxnc3ik"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_latin(word):\n",
        "    return all('LATIN' in unicodedata.name(char) for char in word if char.isalpha())\n",
        "\n",
        "validated_tokens = [word for word in filtered_tokens if is_latin(word)]\n",
        "print(validated_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpeUT2EndHS-",
        "outputId": "79b08e57-9c65-4f61-867f-624dc15d5244"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['22222', 'Hazrat', 'Nizamuddin', 'Mumbai', 'CSMT', 'Rajdhani', 'Express', 'Via', 'Agra', 'Jhansi', 'Bhopal', 'Jalgaon', 'Nashik', 'Kalyan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:-**\n",
        "1. **Tokenization:-** Split the text into individual words or tokens.\n",
        "2. **Filtration:-** Remove punctuation, special characters, and stop words.\n",
        "3. **Script Validation:-** Ensure tokens are in the expected script (Latin in this case)."
      ],
      "metadata": {
        "id": "aoD0Z8BWdVxW"
      }
    }
  ]
}