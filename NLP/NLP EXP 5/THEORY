AIM - Implement N-Gram model for the given text input

THEORY - Our aim is to build an N-Gram model that can predict the next word in a sequence of words. This is a fundamental task in natural language processing, and it has many applications in areas such as language translation, text summarization, and sentiment analysis.
The N-Gram model is based on the Markov chain theory, which states that the probability of a word given its context is independent of the words that come before it. In other words, the probability of a word is only dependent on the previous N-1 words.

Mathematically, this can be represented as:

P(w_n | w_1, w_2, ..., w_(n-1)) = P(w_n | w_(n-1), w_(n-2), ..., w_(n-N+1))

where P(w_n | w_1, w_2, ..., w_(n-1)) is the probability of the word w_n given the context w_1, w_2, ..., w_(n-1), and P(w_n | w_(n-1), w_(n-2), ..., w_(n-N+1)) is the probability of the word w_n given the context w_(n-1), w_(n-2), ..., w_(n-N+1).

CONCLUSION -

In conclusion, the N-Gram model is a simple yet effective way to predict the next word in a sequence of words. By analyzing the context of the previous N-1 words, the model can make accurate predictions about the next word. 
This has many applications in natural language processing, and it is a fundamental building block for more complex language models.

